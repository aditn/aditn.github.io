<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Surface Reconstruction for 3D Point Cloud in Parallel by aditn</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Surface Reconstruction for 3D Point Cloud in Parallel</h1>
      <h2 class="project-tagline">Adit Namdev and Jordan Tick</h2>
    </section>

    <section class="main-content">
<h2>
      <a id="project-proposal" class="anchor" href="https://aditn.github.io/statusUpdate.html">Status Update</a></h2>
      <h2>
<a id="project-proposal" class="anchor" href="#project-proposal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project Proposal</h2>
    
<h3>
  
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary</h3>

<p>We are going to generate and optimize a 3D triangle mesh for point clouds. If we have enough time, our reach goal would be generating the point clouds will be generated using a Xbox Kinect. The computation will be done on an NVIDIA GPU.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<p>Computer graphics revolves around the central concept of meshes. These essentially graph-like structures allow for computation to be highly parallel because usually you want to do an operation over all vertices or all edges, etc. Our goal is to generate a mesh based off a list of vertices. To do this, we must first find the nearby ‘neighborhood’ for each vertex and then connect these nearby points together in a coherent, geometrically sound way. This idea of geometrically sound is more formally known as “manifold”. A manifold mesh is essentially a mesh that doesn’t have any holes in it, no internal faces, and no overlapping edges or parts of the mesh with zero area (a popular example of this error is two pyramids connected at a single point to form an ‘hourglass’). More simply, another definition is that every edge only has a max of two faces (aka triangles) coming off of it. Once we have created a mesh, we must then try to make it optimal! This was essentially the first homework for 15-462 with three operations: 1) subdivision, 2) edge flipping, and 3) edge collapsing. What the goal of these operations are is essentially to smooth out your graph and minimize how many vertices are connected together. Subdivision breaks up a vertex into multiple vertices (finer mesh = smoother), edge flipping can result in fewer connections between certain vertices, and edge collapsing can collapse multiple triangles together (you only need two triangles to represent a plane, no more!).</p>

<p>So what was described above is how to connect vertices based on nearest neighbor search and then improve the quality of your mesh based on a few atomic operations. This works if our point cloud is well defined. If we get to our reach goal of using the connect though, it is possible we may deal with bad point clouds! This idea of a ‘noisy’ mesh could be combatted by A) threshold how far away the nearest neighbor can be to detect ‘outlier’ points and B) use the aforementioned algorithm to smooth out bumpy surfaces by downsampling and then upsampling. That’s the rough idea of our approach! It is nicely segmented into three steps: 1) creation of mesh, 2) improvement of mesh, and 3) using kinect to generate a point cloud. How optimal #1 is determines how long #2 takes. Doing #1 in parallel requires a lot of communication between processes (imagine creating separate meshes and ‘gluing’ them together at their edges. Algorithms like nearest neighbor and noise reduction are highly parallelizable whereas connecting connecting vertices via edges is highly serial (although we could create multiple meshes and ‘glue’ them together at the end as previously stated.</p>

<h3>
<a id="the-challenge" class="anchor" href="#the-challenge" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Challenge</h3>

<p>There are a couple issues for this project. The first difficulty is with regards to memory. GPUs do not have a large amount of space, thus most of the data needs to be in DRAM (especially for large point clouds). The second and more difficult problem is generating partial meshes on different cores and combining them after computation. These two challenges go hand in hand, particularly if the initial point cloud is very large and we cannot completely compute all the sections of the point cloud in one go. We hope to learn how to deal with the memory constraint and improve performance (via load balancing) while keeping correctness.</p>

<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h3>

<p>Code: C++, Cuda, PCL libraries (optional: for point cloud generation and visualization)</p>

<p>Hardware: Microsoft Kinect (for capturing point clouds), Nvidia GPU (might be able to use the GHC servers)</p>

<p>We are planning on doing a triangle mesh. We will be doing more research on this, but Delaunay Triangulation is an option we are thinking about based on an initial search.</p>

<p>^ We will ideally find a paper for a rough outline of how to make sure we build a valid mesh.</p>

<p>^^ We would need access to a kinect as we do not have one ourselves. </p>

<h3>
<a id="goals-and-deliverables" class="anchor" href="#goals-and-deliverables" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Goals and Deliverables</h3>

<p>The nice thing about our project is that it is implemented in steps. Step 1 is to find a way to display points and triangles on our screen (seeing as these are all solid figures, a simple rasterizer could either be found online or written based on old 15462 code). Step 2 is to generate the mesh based on a list of vertices (we expect this step to take the most time). Step 3 is improve the mesh using the operations described in background (Jordan has implemented these before and says they are highly parallelizable so this step should be easy). Step 4 is using a point cloud based off kinect. Before this step, we will just download obj meshes off the internet and delete their edges so that we are working with ideal meshes. Working with a point cloud shouldn’t be too much harder, it just might require correcting for error. Thus our milestones are these:</p>

<ul>
<li>Milestone 1: visualize our points and triangles.</li>
<li>Milestone 2: create a mesh based off a list of vertices.</li>
<li>Milestone 3 (and checkpoint!): create this mesh in parallel</li>
<li>Milestone 4: optimize mesh in parallel</li>
<li>Milestone 5: use kinect to get point clouds</li>
<li>Milestone 6 (and end of project!): Use kinect to create a 3D mesh of a scene.</li>
</ul>

<p>Basically, step 1 and 4 are the easy GUI steps, steps 2 and 3 are where the majority of the project will be, and steps 5 and 6 are cool things to visualize our project with if we have time.</p>

<h3>
<a id="platform-choice" class="anchor" href="#platform-choice" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Platform Choice</h3>

<p>PCL is a very good library for point clouds and is already implemented in C++. We are very familiar with C++ and Cuda which will make our development speed faster. Microsoft Kinect is a great resource for capturing point clouds at medium distances. If we want to capture close range point clouds, we may want to choose a different depth camera (such as DepthSense from SoftKinectic). </p>

<h3>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Schedule</h3>

<ul>
<li>Week 1 (By Nov 7): Visualize our points and triangles and research algorithms for creating mesh.</li>
<li>Week 2 (By Nov 14): Create a mesh based off a list of vertices.</li>
<li>Week 3 (By Nov 21): Work on creating this mesh in parallel.</li>
<li>Week 4 (By Nov 28): Finish creating mesh in parallel. Begin parallelizing the optimization step.</li>
<li>Week 5 (By Dec 5): Finish parallelizing optimization. Use kinect to get point clouds.</li>
<li>Week 6 (By Dec 13): Use kinect to create a 3D mesh of a scene.</li>
</ul>

<h3>
<a id="updated-schedule" class="anchor" href="#updated-schedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Updated Schedule</h3>

<p>We are roughly on track. We are debugging our serial implementation this week, and then will spend the final two weeks parallelizing it. As such, we are leaving the kinect off for now as more of a “reach goal” we will get to if we have time. Our primary goal for now is making our algorithm run in parallel. Because of this, our new schedule looks something like this:</p>

<p>Week 2.5 (By Nov 22): Meet with TAs/professors to fully understand algorithm and link 3rd party libraries (armadillo?,openGL?,cuda?)</p>

<ul>
<li>We will both meet with Keenan to discuss algorithm</li>
<li>Both meet with TAs or graphics on how to get openGL/Armadillo to compile</li>
</ul>

<p>Week 3 (By Nov 28) have serial implementation working and start parallelization</p>

<ul>
<li>Adit focuses on planes and nearest neighbors and distance function</li>
<li>Jordan focuses on cubes and mesh refinement</li>
<li>Both focus on propagating normal vectors since requires parallelizing an MST</li>
</ul>

<p>Week 3.5 (By Dec 1): Rough parallelism of each step</p>

<ul>
<li>Same assignments</li>
</ul>

<p>Week 4 (By Dec 5): identify bottlenecks and attempt possible fixes</p>

<ul>
<li>Meeting up and doing this together</li>
</ul>

<p>Week 5 (By Dec 13): Gather data for various meshes; try some tricks to address bottlenecks</p>

<ul>
<li>Too soon to tell who does what</li>
</ul>

<h3>
<a id="how-were-doing" class="anchor" href="#how-were-doing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How we're doing</h3>

<p>Thus far we have generated point clouds for different shapes (sphere, cubes, etc) for testing. We can view these using Blender. We are following Hugues Hoppe’s thesis on Surface Reconstruction from Unorganized Points to produce a serial algorithm. The majority of this algorithm has been coded, but we are currently in the process of debugging. Although we are still working on the serial implementation, we have discussed how to parallelize sections of the algorithm. For example, for step 3 of the algorithm, we plan on creating a quad-tree to group the points by location and then send that data to a core of the GPU. For step 4, we need to propagate consistent normal vectors between neighboring planes via a Minimum Spanning Tree. Processing the different layers of this tree in parallel will be an interesting task; we were thinking of processing the tree in BFS so that after every few iterations we can somehow ‘fork off’ work since the nodes are independent of each other (since it’s a tree). One final interesting aspect of parallelization is that currently neighbors are doing redundant work (since all cubes share faces with their neighbors); deciding how much or how little we want the neighbors to communicate to each other will have to be determined through experimentation.
One last thing to note is that the nice thing about using voxels in a 3D space is we can fully take advantage of CUDAs 3D indexing since cube(i,j,k) could be mapped to thread.ijk!</p>

<p>We will be able to produce most of the deliverables except for the final task of creating our own point clouds using the Microsoft Kinect. Doing this would be a reach-goal that we would attempt once our parallel implementation is mostly complete. Currently, however, we are behind on implementing the original surface reconstruction algorithm.
Goal 1: Produce a dense surface reconstruction of a point cloud.
Goal 2: Refine mesh to reduce number of edges while keeping the shape intact.
Goal 3 (Reach Goal): Create our own point clouds using a Kinect and test our own algorithm on it.</p>

<h3>
<a id="algorithm" class="anchor" href="#algorithm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Algorithm</h3>

<p>Our algorithm has roughly nine steps:</p>

<ol>
<li>Read points from input file</li>
<li>Find nearest neighbors for every point</li>
<li>Create tangent planes for each point based on its nearest neighbors (approximate surface)</li>
<li>Propagate the normal direction between neighboring planes so that they’re all roughly pointing the same direction</li>
<li>Be able to call a distance function that tells us approximate distance to surface from any arbitrary point in  space</li>
<li>Turn the 3D space encompassing our point cloud into a bunch of cubes/voxels (either implicitly or explicitly)</li>
<li>Create the approximate mesh based on what distance values are calculated for the corners of the cubes</li>
<li>Refine the mesh (likely to be very dense since a point cloud) by deleting unnecessary vertices and trying to have every vertex degree roughly the same</li>
<li>Write mesh to an output file</li>
</ol>

<p>Currently, we’ve serially implemented 1-3,5-7 and 9. We have a rough plan of how we plan to parallelize each stage but we must first debug our serial implementation.</p>

<p>Bug: Difficulty has arisen for step 3 because we were hoping to use the Armadillo C++ library to do PCA for us for finding the normals; we can’t seem to understand how to link this library to our code so instead we found code online that calculates the eigenvalues of a 3x3 matrix and are trying to interface with it from our code.
The benefit of our code is that it is very modular (clearly defined steps). To test our code, we can do the following:</p>

<ol>
<li>Generate and output planes to see if they look correct.</li>
<li>Generate a new point cloud everywhere our distance function is negative (implies inside our mesh) to see if this function is correct.</li>
<li>Output our final mesh to see if it looks like our point cloud.</li>
</ol>

<p>We have left our serial implementation very basic so that we can focus our time on parallelizing it:
Nearest neighbor search: just loop through all points, calculate distance, and keep the closest.
Create tangent planes: get nearest neighbors for each point, approximate centroid and normal.
Distance function: First find nearest point, then get distance to that plane</p>

<p>Turning 3D space into voxels: First we get bounding box for all points, then we divide it into cubes (our bounding box struct)</p>

<p>Creating mesh: for each cube, evaluate the distance function at its corners. No communication between neighbors. We want mesh to be where distance function is zero, so we interpolate values between corners that are positive and corners that are negative to approximate mesh.</p>

<p>Writing mesh to file: Since our cubes don’t communicate to their neighbors at all, this means they generate redundant vertices. Thus, we must sort the vertices and delete duplicates and then also have our edge array index into the vertex array to follow .obj format.</p>

<p>We currently have NOT implemented steps 4 and 8. Step 4 requires us to create an MST/MSF of our points so that nearby points have normal vectors pointing in the same direction; we didn’t want to implement this until we had working normal vectors. Similarly step 8 refines our mesh so we can’t implement this until we have our mesh working. Jordan has taken 15-462 before and one of his homeworks involved mesh simplification, so he is confident this step will be simple once we have the full mesh generated.</p>

<h3>
<a id="poster-session" class="anchor" href="#poster-session" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Poster Session</h3>

<p>Our project is creating a mesh from a point cloud, so in a perfect scenario our demo will include a bunch of examples of this happening! This will mean displaying images of the original point cloud, our reconstructed mesh, and performance graphs. Potentially, as a reach goal, we would have the mesh graphically generated live as a demo to visualize how the points connect.</p>

<h3>
<a id="preliminary-results" class="anchor" href="#preliminary-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Preliminary Results</h3>

<p>We have written a rough serial version of our program and have plans on how to parallelize it. Currently we are in the debugging stage because we were not able to link the C Armadillo API to our code (which means we had to grab some stranger’s code (which we have cited) off the internet and are now trying to understand it so we can call it from our code. The code is for PCA analysis of a matrix, which is needed for generating normals in the point cloud.</p>

<p>Because we are in the debug stage, we do not yet have results to show but we should by the end of the week!</p>

<h3>
<a id="current-issues" class="anchor" href="#current-issues" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Current Issues</h3>

<p>We have a few issues currently. Some of the methods discussed in Hoppe’s paper are vague and research into them have not been too fruitful. We plan on discussing these issues with a Computer Graphics/Vision professor to see if he might be able to shed some light. More pressing, however, include learning to write Makefiles and using an API. We would like to use Armadillo, a C++ linear algebra library, for computing the tangent planes at each point (Step 2). This would allow us to implement the initial serial algorithm quicker and allow us to avoid the hassle of writing new matrix data structures.</p>

      <footer class="site-footer">

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
